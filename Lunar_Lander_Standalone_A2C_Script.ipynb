{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOZkK/9P7LmiAy89Y3Av40w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorkobani/Deep-Reinforcement-Learning/blob/main/Lunar_Lander_Standalone_A2C_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL DEPENDENCIES"
      ],
      "metadata": {
        "id": "ikwYnd8bOvFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx6A4aU4Ogcb"
      },
      "outputs": [],
      "source": [
        "# Installing required dependencies\n",
        "!apt-get update\n",
        "!apt-get install -y swig cmake ffmpeg freeglut3-dev xvfb\n",
        "\n",
        "# Installing more dependencies\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "!pip install \"stable-baselines3[extra]>=2.7.0\"\n",
        "!pip install \"huggingface_sb3>=3.0\"\n",
        "!pip install \"moviepy>=2.2.1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "hhe9VqDuPawX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode"
      ],
      "metadata": {
        "id": "96uTfthCPckR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE GYM ENVIRONMENT AND INSTANTIATE AGENT"
      ],
      "metadata": {
        "id": "WljpsoZJPwVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = A2C(\n",
        "    \"MlpPolicy\",\n",
        "    \"LunarLander-v3\",\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "AjC6Z-5OP0nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATE UNTRAINED AGENT"
      ],
      "metadata": {
        "id": "hvKB2wu6P5jL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's evaluate the un-trained agent, this should be a random agent.\n",
        "eval_env = gym.make(\"LunarLander-v3\")\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model,\n",
        "    eval_env,\n",
        "    n_eval_episodes=20,\n",
        "    deterministic=True,\n",
        ")\n",
        "\n",
        "print(f\"Untrained A2C mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "c8aZ7wZ4QJTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETUP CALLBACK AND TRAIN THE AGENT"
      ],
      "metadata": {
        "id": "cF3JBRotQjO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the callback for logging performance\n",
        "\n",
        "# Create a directory for logs\n",
        "log_dir = \"/tmp/a2c_gym_logs/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Wrap the evaluation environment with a Monitor for the callback.\n",
        "eval_env_monitored = Monitor(gym.make(\"LunarLander-v3\"))\n",
        "\n",
        "# Create the EvalCallback\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env_monitored,\n",
        "    best_model_save_path=os.path.join(log_dir, 'best_model'),\n",
        "    log_path=os.path.join(log_dir, 'results'),\n",
        "    eval_freq=5000, # Evaluate the agent every 5000 steps\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Train the agent and save it\n",
        "print(\"\\n--- Starting A2C Training ---\")\n",
        "# Pass the callback to the learn method\n",
        "model.learn(total_timesteps=int(1e6), log_interval=400, progress_bar=True, callback=eval_callback)\n",
        "model.save(\"a2c_lunar_v3\")\n",
        "del model  # delete trained model to demonstrate loading"
      ],
      "metadata": {
        "id": "Q52LJuWaQkQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " LOAD AND EVALUATE TRAINED AGENT"
      ],
      "metadata": {
        "id": "xy3mvQKwQ1cD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading and Evaluating Final A2C Model ---\")\n",
        "model = A2C.load(\"a2c_lunar_v3\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\n",
        "print(f\"Trained A2C mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "ri6gfuBBRFwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLOT THE RESULTS"
      ],
      "metadata": {
        "id": "rzHoCHEGRLlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the plotting logic\n",
        "\n",
        "print(\"\\n--- Plotting A2C Training Progress ---\")\n",
        "\n",
        "# Construct the correct path to the log file\n",
        "results_path = os.path.join(log_dir, \"results\")\n",
        "log_file = os.path.join(results_path, \"evaluations.npz\")\n",
        "\n",
        "if os.path.exists(log_file):\n",
        "    print(f\"Loading log file from: {log_file}\")\n",
        "    # Load the saved data\n",
        "    data = np.load(log_file)\n",
        "\n",
        "    timesteps = data['timesteps']\n",
        "    mean_rewards = data['results'][:, 0]\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.title(\"Standalone A2C Training Performance on LunarLander-v3\")\n",
        "    plt.xlabel(\"Training Timesteps\")\n",
        "    plt.ylabel(\"Average Reward\")\n",
        "    plt.plot(timesteps, mean_rewards)\n",
        "    plt.axhline(y=200, color='r', linestyle='--', label='Success Threshold (200)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"Log file not found at {log_file}. Cannot plot results.\")"
      ],
      "metadata": {
        "id": "_KA0eRd1ROpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RECORD VIDEO OF TRAINED AGENT"
      ],
      "metadata": {
        "id": "BF03fTz0R4ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Recording Video of Trained A2C Agent ---\")\n",
        "\n",
        "env_id = \"LunarLander-v3\"\n",
        "video_folder = \"logs/videos/\"\n",
        "video_length = 6000\n",
        "os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "# Create the base environment\n",
        "vec_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "name_prefix = f\"a2c-agent-{env_id}\"\n",
        "\n",
        "# Record the video starting at the first step\n",
        "vec_env = VecVideoRecorder(vec_env, video_folder,\n",
        "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
        "                       name_prefix=name_prefix)\n",
        "\n",
        "obs = vec_env.reset()\n",
        "\n",
        "for _ in range(video_length + 1):\n",
        "  action, _state = model.predict(obs, deterministic=True)\n",
        "  obs, _, _, _ = vec_env.step(action)\n",
        "# Save the video\n",
        "vec_env.close()\n",
        "\n",
        "video_filename = f\"{video_folder}{name_prefix}-step-0-to-step-{video_length}.mp4\"\n",
        "\n",
        "if os.path.exists(video_filename):\n",
        "    mp4 = open(video_filename,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "else:\n",
        "    print(f\"Video file not found at {video_filename}\")"
      ],
      "metadata": {
        "id": "MWavZijiR83B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}